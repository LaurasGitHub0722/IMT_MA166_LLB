{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "q97ra-DnzKRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QLME2StsDcd"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -qqq\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyqrDHvndYf1"
      },
      "outputs": [],
      "source": [
        "# Weights and biases\n",
        "\n",
        "import wandb\n",
        "wandb.init(project='Greifen11', name=f\"Greifen11_run01\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.init as init\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import DataParallel\n",
        "from torch.utils.data import Sampler\n",
        "from PIL import Image\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "from torchvision.transforms import Lambda\n",
        "import argparse\n",
        "import copy\n",
        "\n",
        "parser = argparse.ArgumentParser(description='lstm training')\n",
        "parser.add_argument('-g', '--gpu', default=True, type=bool, help='gpu use, default True')\n",
        "parser.add_argument('-s', '--seq', default=10, type=int, help='sequence length, default 10')\n",
        "parser.add_argument('-t', '--train', default=50, type=int, help='train batch size, default 400')\n",
        "parser.add_argument('-v', '--val', default=10, type=int, help='valid batch size, default 10')\n",
        "parser.add_argument('-o', '--opt', default=0, type=int, help='0 for sgd 1 for adam, default 1')\n",
        "parser.add_argument('-m', '--multi', default=1, type=int, help='0 for single opt, 1 for multi opt, default 1')\n",
        "parser.add_argument('-e', '--epo', default=20, type=int, help='epochs to train and val, default 25')\n",
        "parser.add_argument('-w', '--work', default=2, type=int, help='num of workers to use, default 4')\n",
        "parser.add_argument('-f', '--flip', default=1, type=int, help='0 for not flip, 1 for flip, default 0')\n",
        "parser.add_argument('-c', '--crop', default=1, type=int, help='0 rand, 1 cent, 5 five_crop, 10 ten_crop, default 1')\n",
        "parser.add_argument('-l', '--lr', default=5e-4, type=float, help='learning rate for optimizer, default 5e-5')\n",
        "parser.add_argument('--momentum', default=0.9, type=float, help='momentum for sgd, default 0.9')\n",
        "parser.add_argument('--weightdecay', default=5e-4, type=float, help='weight decay for sgd, default 0')\n",
        "parser.add_argument('--dampening', default=0, type=float, help='dampening for sgd, default 0')\n",
        "parser.add_argument('--nesterov', default=False, type=bool, help='nesterov momentum, default False')\n",
        "parser.add_argument('--sgdadjust', default=1, type=int, help='sgd method adjust lr 0 for step 1 for min, default 1')\n",
        "parser.add_argument('--sgdstep', default=5, type=int, help='number of steps to adjust lr for sgd, default 5')\n",
        "parser.add_argument('--sgdgamma', default=0.1, type=float, help='gamma of steps to adjust lr for sgd, default 0.1')\n",
        "parser.add_argument('-fz', '--freeze', default=False, type=bool, help='freeze net, default True')\n",
        "\n",
        "### Hinzugef√ºgt um den Code zum Laufen zu kriegen\n",
        "import sys\n",
        "sys.argv=['']\n",
        "del sys\n",
        "###\n",
        "\n",
        "args = parser.parse_args()\n",
        "wandb.config.update(args)\n",
        "\n",
        "gpu_usg = args.gpu\n",
        "sequence_length = args.seq\n",
        "train_batch_size = args.train\n",
        "val_batch_size = args.val\n",
        "optimizer_choice = args.opt\n",
        "multi_optim = args.multi\n",
        "epochs = args.epo\n",
        "workers = args.work\n",
        "use_flip = args.flip\n",
        "crop_type = args.crop\n",
        "learning_rate = args.lr\n",
        "momentum = args.momentum\n",
        "weight_decay = args.weightdecay\n",
        "dampening = args.dampening\n",
        "use_nesterov = args.nesterov\n",
        "\n",
        "sgd_adjust_lr = args.sgdadjust\n",
        "sgd_step = args.sgdstep\n",
        "sgd_gamma = args.sgdgamma\n",
        "\n",
        "freeze_net = args.freeze\n",
        "\n",
        "num_gpu = torch.cuda.device_count()\n",
        "use_gpu = (torch.cuda.is_available() and gpu_usg)\n",
        "device = torch.device(\"cuda:0\" if use_gpu else \"cpu\")\n",
        "\n",
        "print('number of gpu   : {:6d}'.format(num_gpu))\n",
        "print('sequence length : {:6d}'.format(sequence_length))\n",
        "print('train batch size: {:6d}'.format(train_batch_size))\n",
        "print('valid batch size: {:6d}'.format(val_batch_size))\n",
        "print('optimizer choice: {:6d}'.format(optimizer_choice))\n",
        "print('multiple optim  : {:6d}'.format(multi_optim))\n",
        "print('num of epochs   : {:6d}'.format(epochs))\n",
        "print('num of workers  : {:6d}'.format(workers))\n",
        "print('test crop type  : {:6d}'.format(crop_type))\n",
        "print('whether to flip : {:6d}'.format(use_flip))\n",
        "print('learning rate   : {:.4f}'.format(learning_rate))\n",
        "print('momentum for sgd: {:.4f}'.format(momentum))\n",
        "print('weight decay    : {:.4f}'.format(weight_decay))\n",
        "print('dampening       : {:.4f}'.format(dampening))\n",
        "print('use nesterov    : {:6d}'.format(use_nesterov))\n",
        "print('method for sgd  : {:6d}'.format(sgd_adjust_lr))\n",
        "print('step for sgd    : {:6d}'.format(sgd_step))\n",
        "print('gamma for sgd   : {:.4f}'.format(sgd_gamma))\n",
        "print(\"freeze net      :\",freeze_net)\n",
        "\n",
        "\n",
        "def pil_loader(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        with Image.open(f) as img:\n",
        "            return img.convert('RGB')\n",
        "\n",
        "\n",
        "class GreifenDataset(Dataset):\n",
        "    def __init__(self, file_paths, file_labels, transform=None,\n",
        "                 loader=pil_loader):\n",
        "        self.file_paths = file_paths\n",
        "        self.file_labels = file_labels[:, -1]\n",
        "        self.transform = transform\n",
        "        self.loader = loader\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_names = self.file_paths[index]\n",
        "        labels = self.file_labels[index]\n",
        "        imgs = self.loader(img_names)\n",
        "        if self.transform is not None:\n",
        "            imgs = self.transform(imgs)\n",
        "\n",
        "        return imgs, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "\n",
        "class resnet_lstm(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(resnet_lstm, self).__init__()\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        self.share = torch.nn.Sequential()\n",
        "        self.share.add_module(\"conv1\", resnet.conv1)\n",
        "        self.share.add_module(\"bn1\", resnet.bn1)\n",
        "        self.share.add_module(\"relu\", resnet.relu)\n",
        "        self.share.add_module(\"maxpool\", resnet.maxpool)\n",
        "        self.share.add_module(\"layer1\", resnet.layer1)\n",
        "        self.share.add_module(\"layer2\", resnet.layer2)\n",
        "        self.share.add_module(\"layer3\", resnet.layer3)\n",
        "        self.share.add_module(\"layer4\", resnet.layer4)\n",
        "        self.share.add_module(\"avgpool\", resnet.avgpool)\n",
        "        self.lstm = nn.LSTM(2048, 512, batch_first=True)\n",
        "        #self.fcDropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(512, 7)\n",
        "\n",
        "        init.xavier_normal_(self.lstm.all_weights[0][0])\n",
        "        init.xavier_normal_(self.lstm.all_weights[0][1])\n",
        "        init.xavier_uniform_(self.fc.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 3, 128, 128)\n",
        "        x = self.share.forward(x)\n",
        "        x = x.view(-1, 2048)\n",
        "        x = x.view(-1, sequence_length, 2048)\n",
        "        self.lstm.flatten_parameters()\n",
        "        y, _ = self.lstm(x)\n",
        "        y = y.contiguous().view(-1, 512)\n",
        "        #y = self.fcDropout(y)\n",
        "        y = self.fc(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "def get_useful_start_idx(sequence_length, list_each_length):\n",
        "    count = 0\n",
        "    idx = []\n",
        "    for i in range(len(list_each_length)):\n",
        "        for j in range(count, count + (list_each_length[i] + 1 - sequence_length)):\n",
        "            idx.append(j)\n",
        "        count += list_each_length[i]\n",
        "    return idx\n",
        "\n",
        "\n",
        "def get_data(data_path):\n",
        "    with open(data_path, 'rb') as f:\n",
        "        train_test_paths_labels = pickle.load(f)\n",
        "    train_paths = train_test_paths_labels[0]\n",
        "    val_paths = train_test_paths_labels[1]\n",
        "    test_paths = train_test_paths_labels[2]\n",
        "    train_labels = train_test_paths_labels[3]\n",
        "    val_labels = train_test_paths_labels[4]\n",
        "    test_labels = train_test_paths_labels[5]\n",
        "    train_num_each = train_test_paths_labels[6]\n",
        "    val_num_each = train_test_paths_labels[7]\n",
        "    test_num_each = train_test_paths_labels[8]\n",
        "\n",
        "    print('train_paths  : {:6d}'.format(len(train_paths)))\n",
        "    print('train_labels : {:6d}'.format(len(train_labels)))\n",
        "    print('valid_paths  : {:6d}'.format(len(val_paths)))\n",
        "    print('valid_labels : {:6d}'.format(len(val_labels)))\n",
        "    print('test_paths   : {:6d}'.format(len(test_paths)))\n",
        "    print('test_labels  : {:6d}'.format(len(test_labels)))\n",
        "    #print(test_paths)\n",
        "\n",
        "    train_labels = np.asarray(train_labels, dtype=np.int64)\n",
        "    val_labels = np.asarray(val_labels, dtype=np.int64)\n",
        "    test_labels = np.asarray(test_labels, dtype=np.int64)\n",
        "\n",
        "    train_transforms = None\n",
        "    test_transforms = None\n",
        "\n",
        "    if use_flip == 0:\n",
        "        train_transforms = transforms.Compose([\n",
        "            transforms.RandomCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.3456, 0.2281, 0.2233], [0.2528, 0.2135, 0.2104])\n",
        "        ])\n",
        "    elif use_flip == 1:\n",
        "        train_transforms = transforms.Compose([\n",
        "            transforms.RandomCrop(128),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.RandomGrayscale(), #\tRandomly convert image to grayscale with a probability of p (default 0.1).\n",
        "            transforms.Normalize([0.3456, 0.2281, 0.2233], [0.2528, 0.2135, 0.2104])\n",
        "        ])\n",
        "\n",
        "    if crop_type == 0:\n",
        "        test_transforms = transforms.Compose([\n",
        "            transforms.RandomCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.3456, 0.2281, 0.2233], [0.2528, 0.2135, 0.2104])\n",
        "        ])\n",
        "    elif crop_type == 1:\n",
        "        test_transforms = transforms.Compose([\n",
        "            transforms.CenterCrop(128),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.3456, 0.2281, 0.2233], [0.2528, 0.2135, 0.2104])\n",
        "        ])\n",
        "    elif crop_type == 5:\n",
        "        test_transforms = transforms.Compose([\n",
        "            transforms.FiveCrop(224),\n",
        "            Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
        "            Lambda(\n",
        "                lambda crops: torch.stack(\n",
        "                    [transforms.Normalize([0.3456, 0.2281, 0.2233], [0.2528, 0.2135, 0.2104])(crop) for crop in crops]))\n",
        "        ])\n",
        "    elif crop_type == 10:\n",
        "        test_transforms = transforms.Compose([\n",
        "            transforms.Resize((250, 250)),\n",
        "            transforms.TenCrop(224),\n",
        "            Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
        "            Lambda(\n",
        "                lambda crops: torch.stack(\n",
        "                    [transforms.Normalize([0.3456, 0.2281, 0.2233], [0.2528, 0.2135, 0.2104])(crop) for crop in crops]))\n",
        "        ])\n",
        "\n",
        "    train_dataset = GreifenDataset(train_paths, train_labels, train_transforms)\n",
        "    val_dataset = GreifenDataset(val_paths, val_labels, test_transforms)\n",
        "    test_dataset = GreifenDataset(test_paths, test_labels, test_transforms)\n",
        "\n",
        "    return train_dataset, train_num_each, val_dataset, val_num_each, test_dataset, test_num_each\n",
        "\n",
        "\n",
        "# Â∫èÂàóÈááÊ†∑sampler\n",
        "class SeqSampler(Sampler):\n",
        "    def __init__(self, data_source, idx):\n",
        "        super().__init__(data_source)\n",
        "        self.data_source = data_source\n",
        "        self.idx = idx\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx)\n",
        "\n",
        "\n",
        "def train_model(train_dataset, train_num_each, val_dataset, val_num_each):\n",
        "    num_train = len(train_dataset)\n",
        "    num_val = len(val_dataset)\n",
        "\n",
        "    train_useful_start_idx = get_useful_start_idx(sequence_length, train_num_each)\n",
        "    val_useful_start_idx = get_useful_start_idx(sequence_length, val_num_each)\n",
        "\n",
        "    num_train_we_use = len(train_useful_start_idx) // num_gpu * num_gpu\n",
        "    num_val_we_use = len(val_useful_start_idx) // num_gpu * num_gpu\n",
        "    # num_train_we_use = 8000\n",
        "    # num_val_we_use = 800\n",
        "\n",
        "    train_we_use_start_idx = train_useful_start_idx[0:num_train_we_use]\n",
        "    val_we_use_start_idx = val_useful_start_idx[0:num_val_we_use]\n",
        "\n",
        "    #    np.random.seed(0)\n",
        "    # np.random.shuffle(train_we_use_start_idx)\n",
        "    train_idx = []\n",
        "    for i in range(num_train_we_use):\n",
        "        for j in range(sequence_length):\n",
        "            train_idx.append(train_we_use_start_idx[i] + j)\n",
        "\n",
        "    val_idx = []\n",
        "    for i in range(num_val_we_use):\n",
        "        for j in range(sequence_length):\n",
        "            val_idx.append(val_we_use_start_idx[i] + j)\n",
        "\n",
        "    num_train_all = len(train_idx)\n",
        "    num_val_all = len(val_idx)\n",
        "    print('num of train dataset: {:6d}'.format(num_train))\n",
        "    print('num train start idx : {:6d}'.format(len(train_useful_start_idx)))\n",
        "    print('last idx train start: {:6d}'.format(train_useful_start_idx[-1]))\n",
        "    print('num of train we use : {:6d}'.format(num_train_we_use))\n",
        "    print('num of all train use: {:6d}'.format(num_train_all))\n",
        "    print('num of valid dataset: {:6d}'.format(num_val))\n",
        "    print('num valid start idx : {:6d}'.format(len(val_useful_start_idx)))\n",
        "    print('last idx valid start: {:6d}'.format(val_useful_start_idx[-1]))\n",
        "    print('num of valid we use : {:6d}'.format(num_val_we_use))\n",
        "    print('num of all valid use: {:6d}'.format(num_val_all))\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=val_batch_size,\n",
        "        sampler=SeqSampler(val_dataset, val_idx),\n",
        "        num_workers=workers,\n",
        "        pin_memory=False\n",
        "    )\n",
        "    model = resnet_lstm()\n",
        "    #model.load_state_dict(torch.load('/content/drive/MyDrive/MA_LL/Train01/lstm_epoch_14_length_10_opt_0_mulopt_1_flip_1_crop_1_batch_50_train_9924_val_9485.pth'))\n",
        "    if use_gpu:\n",
        "        model = DataParallel(model)\n",
        "        model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(size_average=False)\n",
        "\n",
        "    optimizer = None\n",
        "    exp_lr_scheduler = None\n",
        "\n",
        "    if multi_optim == 0:\n",
        "        if optimizer_choice == 0:\n",
        "            optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, dampening=dampening,\n",
        "                                  weight_decay=weight_decay, nesterov=use_nesterov, )\n",
        "            if sgd_adjust_lr == 0:\n",
        "                exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=sgd_adjust_lr, gamma=sgd_gamma)\n",
        "            elif sgd_adjust_lr == 1:\n",
        "                exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
        "        elif optimizer_choice == 1:\n",
        "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    elif multi_optim == 1:\n",
        "        if optimizer_choice == 0:\n",
        "            optimizer = optim.SGD([\n",
        "                {'params': model.module.share.parameters()},\n",
        "                {'params': model.module.lstm.parameters(), 'lr': learning_rate},\n",
        "                {'params': model.module.fc.parameters(), 'lr': learning_rate},\n",
        "            ], lr=learning_rate / 10, momentum=momentum, dampening=dampening,\n",
        "                  weight_decay=weight_decay, nesterov=use_nesterov)\n",
        "            if sgd_adjust_lr == 0:\n",
        "                  exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=sgd_adjust_lr, gamma=sgd_gamma)\n",
        "            elif sgd_adjust_lr == 1:\n",
        "                  exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
        "        elif optimizer_choice == 1:\n",
        "            optimizer = optim.Adam([\n",
        "                {'params': model.module.share.parameters()},\n",
        "                {'params': model.module.lstm.parameters(), 'lr': learning_rate},\n",
        "                {'params': model.module.fc.parameters(), 'lr': learning_rate},\n",
        "            ], lr=learning_rate / 10)\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.module.state_dict())\n",
        "    best_val_accuracy = 0.0\n",
        "    correspond_train_acc = 0.0\n",
        "    best_epoch = 0\n",
        "\n",
        "    record_np = np.zeros([epochs, 4])\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # np.random.seed(epoch)\n",
        "        np.random.shuffle(train_we_use_start_idx)\n",
        "        train_idx = []\n",
        "        for i in range(num_train_we_use):\n",
        "            for j in range(sequence_length):\n",
        "                train_idx.append(train_we_use_start_idx[i] + j)\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=train_batch_size,\n",
        "            sampler=SeqSampler(train_dataset, train_idx),\n",
        "            num_workers=workers,\n",
        "            pin_memory=False\n",
        "        )\n",
        "\n",
        "        # Sets the module in training mode.\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_corrects = 0\n",
        "        batch_progress = 0.0\n",
        "        train_start_time = time.time()\n",
        "        for data in train_loader:           \n",
        "            optimizer.zero_grad()\n",
        "             # ÈáäÊîæÊòæÂ≠ò\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            if use_gpu:\n",
        "                inputs, labels = data[0].to(device), data[1].to(device)\n",
        "                labels = labels[(sequence_length - 1)::sequence_length]\n",
        "            else:\n",
        "                inputs, labels = data[0], data[1]\n",
        "                labels = labels[(sequence_length - 1)::sequence_length]\n",
        "\n",
        "            inputs = inputs.view(-1, sequence_length, 3, 128, 128)\n",
        "\n",
        "            outputs = model.forward(inputs)\n",
        "            outputs = outputs[sequence_length - 1::sequence_length]\n",
        "\n",
        "            _, preds = torch.max(outputs.data, 1)            \n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.data.item()\n",
        "            batch_corrects = torch.sum(preds == labels.data)\n",
        "            train_corrects += batch_corrects\n",
        "\n",
        "            batch_acc=float(batch_corrects)/train_batch_size*sequence_length\n",
        "\n",
        "            batch_progress += 1\n",
        "            if batch_progress*train_batch_size >= num_train_all:\n",
        "                percent = 100.0\n",
        "                print('Batch progress: %s [%d/%d] Batch acc:%.2f' % (str(percent) + '%', num_train_all, num_train_all, batch_acc), end='\\n')\n",
        "            else:\n",
        "                percent = round(batch_progress*train_batch_size / num_train_all * 100, 2)\n",
        "                print('Batch progress: %s [%d/%d] Batch acc:%.2f' % (str(percent) + '%', batch_progress*train_batch_size, num_train_all, batch_acc), end='\\r')\n",
        "\n",
        "        # Log the network weight histograms (optional)\n",
        "        wandb.watch(model)\n",
        "\n",
        "        train_elapsed_time = time.time() - train_start_time\n",
        "        train_accuracy = float(train_corrects) / float(num_train_all)*sequence_length\n",
        "        train_average_loss = train_loss / num_train_all*sequence_length\n",
        "\n",
        "        \n",
        "\n",
        "        # Sets the module in evaluation mode.\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_corrects = 0\n",
        "        val_start_time = time.time()\n",
        "        val_progress = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in val_loader:\n",
        "                # ÈáäÊîæÊòæÂ≠ò\n",
        "                torch.cuda.empty_cache()\n",
        "                if use_gpu:\n",
        "                    inputs, labels = data[0].to(device), data[1].to(device)\n",
        "                    labels = labels[(sequence_length - 1)::sequence_length]\n",
        "                else:\n",
        "                    inputs, labels = data[0], data[1]\n",
        "                    labels = labels[(sequence_length - 1)::sequence_length]\n",
        "\n",
        "                if crop_type == 0 or crop_type == 1:\n",
        "                    inputs = inputs.view(-1, sequence_length, 3, 128, 128)\n",
        "                    outputs = model.forward(inputs)\n",
        "                elif crop_type == 5:\n",
        "                    inputs = inputs.permute(1, 0, 2, 3, 4).contiguous()\n",
        "                    inputs = inputs.view(-1, 3, 224, 224)\n",
        "                    outputs = model.forward(inputs)\n",
        "                    outputs = outputs.view(5, -1, 7)\n",
        "                    outputs = torch.mean(outputs, 0)\n",
        "                elif crop_type == 10:\n",
        "                    inputs = inputs.permute(1, 0, 2, 3, 4).contiguous()\n",
        "                    inputs = inputs.view(-1, 3, 224, 224)\n",
        "                    outputs = model.forward(inputs)\n",
        "                    outputs = outputs.view(10, -1, 7)\n",
        "                    outputs = torch.mean(outputs, 0)\n",
        "\n",
        "                outputs = outputs[sequence_length - 1::sequence_length]\n",
        "\n",
        "                _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.data.item()\n",
        "                val_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "                val_progress += 1\n",
        "                if val_progress*val_batch_size >= num_val_all:\n",
        "                    percent = 100.0\n",
        "                    print('Val progress: %s [%d/%d]' % (str(percent) + '%', num_val_all, num_val_all), end='\\n')\n",
        "                else:\n",
        "                    percent = round(val_progress*val_batch_size / num_val_all * 100, 2)\n",
        "                    print('Val progress: %s [%d/%d]' % (str(percent) + '%', val_progress*val_batch_size, num_val_all), end='\\r')\n",
        "\n",
        "        val_elapsed_time = time.time() - val_start_time\n",
        "        val_accuracy = float(val_corrects) / float(num_val_we_use)\n",
        "        val_average_loss = val_loss / num_val_we_use\n",
        "        print('epoch: {:4d}'\n",
        "              ' train in: {:2.0f}m{:2.0f}s'\n",
        "              ' train loss: {:4.4f}'\n",
        "              ' train accu: {:.4f}'\n",
        "              ' valid in: {:2.0f}m{:2.0f}s'\n",
        "              ' valid loss: {:4.4f}'\n",
        "              ' valid accu: {:.4f}'\n",
        "              .format(epoch,\n",
        "                      train_elapsed_time // 60,\n",
        "                      train_elapsed_time % 60,\n",
        "                      train_average_loss,\n",
        "                      train_accuracy,\n",
        "                      val_elapsed_time // 60,\n",
        "                      val_elapsed_time % 60,\n",
        "                      val_average_loss,\n",
        "                      val_accuracy))\n",
        "\n",
        "        if optimizer_choice == 0:\n",
        "            if sgd_adjust_lr == 0:\n",
        "                exp_lr_scheduler.step()\n",
        "            elif sgd_adjust_lr == 1:\n",
        "                exp_lr_scheduler.step(val_average_loss)\n",
        "\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            correspond_train_acc = train_accuracy\n",
        "            best_model_wts = copy.deepcopy(model.module.state_dict())\n",
        "            best_epoch = epoch\n",
        "        if val_accuracy == best_val_accuracy:\n",
        "            if train_accuracy > correspond_train_acc:\n",
        "                correspond_train_acc = train_accuracy\n",
        "                best_model_wts = copy.deepcopy(model.module.state_dict())\n",
        "                best_epoch = epoch\n",
        "\n",
        "        record_np[epoch, 0] = train_accuracy\n",
        "        record_np[epoch, 1] = train_average_loss\n",
        "        record_np[epoch, 2] = val_accuracy\n",
        "        record_np[epoch, 3] = val_average_loss\n",
        "\n",
        "        save_val = int(\"{:4.0f}\".format(best_val_accuracy * 10000))\n",
        "        save_train = int(\"{:4.0f}\".format(correspond_train_acc * 10000))\n",
        "        model_name = \"/content/drive/MyDrive/MA_LL/lstm\" \\\n",
        "                     + \"_epoch_\" + str(best_epoch) \\\n",
        "                     + \"_length_\" + str(sequence_length) \\\n",
        "                     + \"_opt_\" + str(optimizer_choice) \\\n",
        "                     + \"_mulopt_\" + str(multi_optim) \\\n",
        "                     + \"_flip_\" + str(use_flip) \\\n",
        "                     + \"_crop_\" + str(crop_type) \\\n",
        "                     + \"_batch_\" + str(train_batch_size) \\\n",
        "                     + \"_train_\" + str(save_train) \\\n",
        "                     + \"_val_\" + str(save_val) \\\n",
        "                     + \".pth\"\n",
        "\n",
        "        torch.save(best_model_wts, model_name)\n",
        "        print(\"best_epoch\",str(best_epoch))\n",
        "\n",
        "        record_name = \"/content/drive/MyDrive/MA_LL/lstm\" \\\n",
        "                      + \"_epoch_\" + str(best_epoch) \\\n",
        "                      + \"_length_\" + str(sequence_length) \\\n",
        "                      + \"_opt_\" + str(optimizer_choice) \\\n",
        "                      + \"_mulopt_\" + str(multi_optim) \\\n",
        "                      + \"_flip_\" + str(use_flip) \\\n",
        "                      + \"_crop_\" + str(crop_type) \\\n",
        "                      + \"_batch_\" + str(train_batch_size) \\\n",
        "                      + \"_train_\" + str(save_train) \\\n",
        "                      + \"_val_\" + str(save_val) \\\n",
        "                      + \".npy\"\n",
        "        np.save(record_name, record_np)\n",
        "\n",
        "        # Log the loss and accuracy values at the end of each epoch\n",
        "        wandb.log({\n",
        "            \"Epoch\": epoch,\n",
        "            \"Train Loss\": train_average_loss,\n",
        "            \"Train Acc\": train_accuracy,\n",
        "            \"Valid Loss\": val_average_loss,\n",
        "            \"Valid Acc\": val_accuracy})\n",
        "\n",
        "    print('best accuracy: {:.4f} cor train accu: {:.4f}'.format(best_val_accuracy, correspond_train_acc))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    train_dataset, train_num_each, val_dataset, val_num_each, _, _ = get_data('/content/drive/MyDrive/MA_LL/Datensatz_1143/train_val_test_paths_labels.pkl')\n",
        "    train_model(train_dataset, train_num_each, val_dataset, val_num_each)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "print('Done')\n",
        "print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "pytorch1.2.0_train_singlenet_phase.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}